{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e33880b-b67d-4d15-95af-e5ab15be6bcb",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d66ce4-6e80-40fa-afd1-73752d4d7d69",
   "metadata": {},
   "source": [
    "Simple Linear Regression:\n",
    "\n",
    "Definition: Simple linear regression involves a single independent variable (predictor) to predict the value of a dependent variable (response).\n",
    "\n",
    "Model Equation: \n",
    "  The equation for simple linear regression is:\n",
    " y = a0 + a1x + ε \n",
    " \n",
    " Where,\n",
    "\n",
    " a0= It is the intercept of the Regression line (can be obtained putting x=0)\n",
    " \n",
    " a1= It is the slope of the regression line, which tells whether the line is increasing or decreasing.\n",
    " \n",
    " ε = The error term. (For a good model it will be negligible)\n",
    " \n",
    " Example: Predicting a person's weight based on their height. Here, weight is the dependent variable, and height is the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb57b5f-f155-4e8d-a4fd-b5b05d960020",
   "metadata": {},
   "source": [
    "Multiple Linear Regression\n",
    "Definition: Multiple linear regression involves two or more independent variables to predict the value of a dependent variable.\n",
    "\n",
    "Model Equation: The equation for multiple linear regression is:\n",
    "𝑌 = 𝛽0 + 𝛽1𝑋1 + 𝛽2𝑋2 + ⋯ +𝛽𝑛𝑋𝑛 + 𝜖\n",
    "\n",
    "Y is the dependent variable.\n",
    "\n",
    "𝛽0 is the y-intercept.\n",
    "\n",
    "𝛽1,𝛽2,…,𝛽𝑛 are the coefficients representing the change in 𝑌 for a one-unit change in the respective 𝑋 variables.\n",
    "\n",
    "𝑋1,𝑋2,…,𝑋𝑛 are the independent variables.\n",
    "\n",
    "ϵ is the error term.\n",
    "\n",
    "Example: Predicting a person's weight based on their height, age, and gender. Here, weight is the dependent variable, and height, age, and gender are the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a72860-7d0f-4220-b137-23a26760dc07",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b86896-e0ac-432b-9fd0-85d6753b62ae",
   "metadata": {},
   "source": [
    "Assumptions of Linear Regression\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable is linear.\n",
    "\n",
    "Independence: The residuals (errors) are independent. This means there is no correlation between consecutive residuals.\n",
    "\n",
    "Homoscedasticity: The residuals have constant variance at every level of the independent variable(s).\n",
    "\n",
    "Normality: The residuals of the model are normally distributed.\n",
    "\n",
    "No Multicollinearity: In the case of multiple linear regression, the independent variables are not too highly correlated with each other.\n",
    "\n",
    "\n",
    "Checking Assumptions\n",
    "\n",
    "Linearity\n",
    "\n",
    "Visual Inspection: Plot the observed values versus the predicted values or the independent variables. The relationship should appear linear.\n",
    "\n",
    "Residual Plots: Plot residuals against the predicted values. There should be no obvious patterns (like curves).\n",
    "\n",
    "Independence\n",
    "\n",
    "Durbin-Watson Test: This statistical test can be used to detect the presence of autocorrelation at lag 1 in the residuals.\n",
    "\n",
    "Plot Residuals: Plot residuals over time (if data is time series). Look for randomness without patterns.\n",
    "\n",
    "Homoscedasticity\n",
    "\n",
    "Residual Plots: Plot residuals against the predicted values. The spread of residuals should be approximately constant across all levels of the independent variables.\n",
    "\n",
    "Breusch-Pagan Test: A statistical test used to detect heteroscedasticity.\n",
    "\n",
    "Normality\n",
    "\n",
    "Q-Q Plot: Plot the quantiles of the residuals against the quantiles of a normal distribution. The points should lie approximately along a straight line.\n",
    "\n",
    "Shapiro-Wilk Test: A statistical test for normality of the residuals.\n",
    "\n",
    "Histogram: Create a histogram of the residuals to check if they follow a normal distribution.\n",
    "\n",
    "No Multicollinearity\n",
    "\n",
    "Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF values above 10 indicate high multicollinearity.\n",
    "\n",
    "Correlation Matrix: Check the pairwise correlations between independent variables. High correlations (above 0.8 or below -0.8) indicate potential multicollinearity issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915024b-68ca-4202-b614-c469e1a55f79",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a2d37-2e71-4703-a932-7539a38733e7",
   "metadata": {},
   "source": [
    "Interpretation of the Slope and Intercept\n",
    "Intercept ( 𝛽0 ):\n",
    "\n",
    "The intercept is the value of the dependent variable 𝑌 when all independent variables 𝑋 are zero.\n",
    "\n",
    "It represents the point at which the regression line crosses the Y-axis.\n",
    "\n",
    "In some cases, the intercept might not have a meaningful interpretation, especially if setting all independent variables to zero is not practical or meaningful.\n",
    "\n",
    "Slope (𝛽1):\n",
    "\n",
    "The slope is the change in the dependent variable 𝑌 for a one-unit change in the independent variable 𝑋, holding all other variables constant.\n",
    "\n",
    "It indicates the direction (positive or negative) and the strength of the relationship between 𝑋 and 𝑌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c446a6-0b9a-49b4-9e56-382ef114b679",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd9a3f-c263-4df6-9745-aba17258bed0",
   "metadata": {},
   "source": [
    "Concept of Gradient Descent\n",
    "\n",
    "Gradient descent works by following these steps:\n",
    "\n",
    "Initialize Parameters: Start with initial values for the model parameters (weights).\n",
    "\n",
    "Compute the Gradient: Calculate the gradient (partial derivatives) of the cost function with respect to each parameter. The gradient points in the direction of the steepest increase in the cost function.\n",
    "\n",
    "Update Parameters: Adjust the parameters in the opposite direction of the gradient to decrease the cost function. This is done using the learning rate, a small positive value that controls the size of the steps taken towards the minimum.\n",
    "\n",
    "Repeat: Iterate the process of computing the gradient and updating the parameters until the cost function converges to a minimum (or stops changing significantly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeacbfb0-d758-4305-be95-dbba8efd7450",
   "metadata": {},
   "source": [
    "Use in Machine Learning\n",
    "\n",
    "Gradient descent is widely used in machine learning, particularly for training models like linear regression, logistic regression, neural networks, and more. Here's how it's typically used:\n",
    "\n",
    "Linear Regression: To minimize the mean squared error (MSE) between the predicted and actual values.\n",
    "\n",
    "Logistic Regression: To minimize the binary cross-entropy loss, enabling classification tasks.\n",
    "\n",
    "Neural Networks: To minimize a complex loss function (e.g., cross-entropy for classification tasks) by adjusting the weights and biases of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0dd2d8-cd86-4d5e-a92c-bfca57e19cba",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ebbf1-14c4-46c3-859d-22174a1b1212",
   "metadata": {},
   "source": [
    "Multiple linear regression (MLR) is an extension of simple linear regression (SLR) that involves multiple independent variables (predictors) to predict a single dependent variable (response). Both models aim to establish a linear relationship between the predictors and the response, but they differ in the number of predictors they include.\n",
    "\n",
    "Multiple Linear Regression Model\n",
    "Definition: Multiple linear regression models the relationship between a dependent variable and multiple independent variables by fitting a linear equation to the observed data.\n",
    "\n",
    "Differences between Multiple Linear Regression and Simple Linear Regression\n",
    "Number of Independent Variables:\n",
    "\n",
    "SLR: Only one independent variable.\n",
    "MLR: Two or more independent variables.\n",
    "Model Complexity:\n",
    "\n",
    "SLR: Simpler model with one predictor, easier to visualize (as a straight line).\n",
    "MLR: More complex model with multiple predictors, harder to visualize in higher dimensions.\n",
    "Equation:\n",
    "\n",
    "SLR: 𝑌=𝛽0+𝛽1𝑋+𝜖\n",
    "\n",
    "MLR: 𝑌= 𝛽0+𝛽1𝑋1+𝛽2𝑋2+⋯+𝛽𝑛𝑋𝑛+𝜖\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "SLR: The slope (𝛽1) represents the change in 𝑌 for a one-unit change in 𝑋.\n",
    "\n",
    "MLR: Each coefficient (𝛽𝑗) represents the change in 𝑌 for a one-unit change in 𝑋𝑗 , holding all other variables constant.\n",
    "\n",
    "Application:\n",
    "\n",
    "SLR: Used when there's a single factor affecting the dependent variable.\n",
    "\n",
    "MLR: Used when multiple factors influence the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a912d-dc33-4d9c-843c-67c713cfca71",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f37a18-8adc-4299-9a73-bde12bd184ed",
   "metadata": {},
   "source": [
    "Multicollinearity refers to a situation in multiple linear regression where two or more independent variables are highly correlated, meaning they provide redundant information about the relationship with the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd89937-7458-4e5d-81c2-6f546a892844",
   "metadata": {},
   "source": [
    "Detecting Multicollinearity\n",
    "\n",
    "Correlation Matrix: A correlation matrix displays the pairwise correlations between the independent variables. High correlation coefficients (above 0.8 or below -0.8) indicate potential multicollinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity. The VIF for each independent variable is calculated as:\n",
    "VIF(𝑋𝑖)=1/1−𝑅𝑖^2\n",
    "\n",
    "Tolerance: Tolerance is the reciprocal of VIF. It indicates the proportion of variance in the predictor that is not explained by other predictors. A tolerance value less than 0.1 suggests high multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity\n",
    "\n",
    "Remove Highly Correlated Predictors: Identify and remove one of the highly correlated variables to reduce redundancy.\n",
    "\n",
    "Combine Predictors: Combine correlated variables into a single predictor using methods like Principal Component Analysis (PCA) or by creating an index.\n",
    "\n",
    "Ridge Regression: Use ridge regression, which adds a penalty term to the cost function to shrink the regression coefficients. This can help to handle multicollinearity by introducing bias but reducing variance.\n",
    "\n",
    "Partial Least Squares Regression: This technique reduces the predictors to a smaller set of uncorrelated components and then performs linear regression on these components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce62cb-4df8-40e1-8cc7-ac1a3104d305",
   "metadata": {},
   "source": [
    "Q7. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c5206-cecf-4c6a-b73a-4cc0c22a894f",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis in which the relationship between the independent variable 𝑋 and the dependent variable Y is modeled as an n-th degree polynomial. While linear regression fits a straight line to the data, polynomial regression can fit a curved line by including higher-order terms of the independent variable\n",
    "\n",
    "Advantages of Polynomial Regression\n",
    "\n",
    "Flexibility: Polynomial regression can model nonlinear relationships between the independent and dependent variables, providing a better fit for datasets where the relationship is not linear.\n",
    "\n",
    "Better Fit for Curved Data: For data that follows a curved trend, polynomial regression can capture the underlying patterns more effectively than linear regression.\n",
    "\n",
    "More Accurate Predictions: By fitting the data more closely, polynomial regression can improve the accuracy of predictions for datasets with nonlinear relationships.\n",
    "\n",
    "\n",
    "Disadvantages of Polynomial Regression\n",
    "\n",
    "Overfitting: Higher-degree polynomials can lead to overfitting, where the model fits the training data very well but performs poorly on unseen data. Overfitting occurs when the model captures noise and random fluctuations in the data rather than the underlying trend.\n",
    "\n",
    "Complexity: Polynomial regression models can become complex and difficult to interpret, especially as the degree of the polynomial increases.\n",
    "\n",
    "Extrapolation Issues: Polynomial regression models can produce unrealistic predictions outside the range of the observed data, particularly for high-degree polynomials.\n",
    "\n",
    "Computationally Intensive: Fitting high-degree polynomials can be computationally more intensive and less efficient compared to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b4321-9d3b-4f51-86f1-84da3b3a12f0",
   "metadata": {},
   "source": [
    "Situations to Prefer Polynomial Regression\n",
    "\n",
    "Nonlinear Relationships: When the relationship between the independent and dependent variables is inherently nonlinear, polynomial regression can provide a better fit. For example, modeling the growth rate of a population over time, which might follow a quadratic or cubic trend.\n",
    "\n",
    "Data Patterns with Curvature: When visual inspection of a scatter plot suggests a curved pattern, polynomial regression can capture the curvature. For instance, the relationship between the dosage of a drug and its effectiveness might exhibit a nonlinear pattern.\n",
    "\n",
    "Complex Interactions: When the effect of an independent variable on the dependent variable changes at different levels of the variable, polynomial regression can help model these interactions. An example is modeling the trajectory of an object under gravity, where the path is parabolic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd45e9-e0c3-4f2e-abcc-74e32228fc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
